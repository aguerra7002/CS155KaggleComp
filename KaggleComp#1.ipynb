{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Seed the random number generator:\n",
    "np.random.seed(1)\n",
    "\n",
    "def load_data(filename, skiprows = 1):\n",
    "    \"\"\"\n",
    "    Function loads data stored in the file filename and returns it as a numpy ndarray.\n",
    "    \n",
    "    Inputs:\n",
    "        filename: given as a string.\n",
    "        \n",
    "    Outputs:\n",
    "        Data contained in the file, returned as a numpy ndarray\n",
    "    \"\"\"\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load all the data\n",
    "all = load_data('train_2008.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### FEATURE ENGINEERING HERE #######\n",
    "# Below, we transform the real valued \n",
    "# data from the dataset to categorical \n",
    "# data if it should be. This should \n",
    "# vastly improve model performance.\n",
    "########################################\n",
    "import csv\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Split into features and labels\n",
    "Y = all[:, -1]\n",
    "X = all[:, :-1]\n",
    "\n",
    "featureNames = None\n",
    "with open('train_2008.csv', 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        featureNames = row\n",
    "        break\n",
    "\n",
    "# Delete all the columns which are identical among all data points, these only hurt our model \n",
    "shouldDelArr = [\"id\", \"QSTNUM\"]\n",
    "shouldDelete = np.all(X == X[0,:], axis = 0)\n",
    "for i in range(X.shape[1]):\n",
    "    ib = X.shape[1] - i - 1\n",
    "    if ((shouldDelete[ib] == True) or (featureNames[ib] in shouldDelArr)):\n",
    "        # delete the i-th columnif \n",
    "        X = np.delete(X, ib, 1)\n",
    "        featureNames = np.delete(featureNames, ib)\n",
    "\n",
    "newX = np.empty((X.shape[0],1))\n",
    "\n",
    "# These are the features which are NOT categorical; the rest I deemed were.\n",
    "real_valued_feats = [\"HWHHWGT\", \"GTCBSA\", \"GTCO\", \"PEAGE\", \"PEHRUSL1\", \"PEHRUSL2\", \"PUHROFF2\", \"PUHROT2\", \"PEHRACT2\", \n",
    "                     \"PELAYDUR\", \"PELKDUR\", \"PRUNEDUR\", \"PEERNHRO\", \"PEERNWKP\", \"PRNMCHLD\", \"PEHGCOMP\", \"PECYC\", \"PWCMPWGT\"]\n",
    "\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    feature = np.reshape(X[:, i], (X.shape[0], 1))\n",
    "    \n",
    "    num_distinct_negs = 0\n",
    "    min_neg = 0\n",
    "    distinct_negs = []\n",
    "    for val in feature:\n",
    "        if val < 0:\n",
    "            if val not in distinct_negs:\n",
    "                distinct_negs.append(val)\n",
    "                num_distinct_negs += 1\n",
    "            if val < min_neg:\n",
    "                min_neg = val\n",
    "    \n",
    "    #print(feature.shape)\n",
    "    if featureNames[i] in real_valued_feats:\n",
    "        newX = np.append(newX, feature, axis = 1)\n",
    "    else:\n",
    "        feature -= min_neg\n",
    "        cat = keras.utils.np_utils.to_categorical(X[:, 0])\n",
    "        \n",
    "        # delete the zero columns\n",
    "        zero_cols = np.any(cat, axis=0)\n",
    "        for j in range(zero_cols.shape[0]):\n",
    "            jb = zero_cols.shape[0] - j - 1\n",
    "            if zero_cols[jb] == False:\n",
    "                np.delete(cat, jb, axis=1)\n",
    "                \n",
    "        # Delete the negative cols\n",
    "        np.delete(cat, np.s_[0:num_distinct_negs], 1) \n",
    "        \n",
    "        # print(test.shape, newX.shape)\n",
    "        newX = np.append(newX, cat, axis = 1)\n",
    "        \n",
    "        \n",
    "# Delete the placeholder column and transpose\n",
    "newX = np.delete(newX, 0, 1)\n",
    "print(newX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into train and validation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(rows_train, rows_test):\n",
    "    columns_train  = np.transpose(rows_train)\n",
    "    columns_test = np.transpose(rows_test)\n",
    "    for i in range(columns_train.shape[0]):\n",
    "        col = columns_train[i]\n",
    "        mean = np.mean(col)\n",
    "        stddev = np.std(col)\n",
    "        if (stddev == 0):\n",
    "            stddev = 1\n",
    "        columns_train[i] = (col - mean) / stddev\n",
    "        columns_test[i] = (columns_test[i] - mean) / stddev\n",
    "    return np.transpose(columns_train), np.transpose(columns_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_1d = Y_train.copy()\n",
    "Y_test_1d = Y_test.copy()\n",
    "\n",
    "# First convert to real labels\n",
    "Y_train = keras.utils.np_utils.to_categorical(Y_train)\n",
    "Y_test = keras.utils.np_utils.to_categorical(Y_test)\n",
    "\n",
    "\n",
    "X_train, X_test = normalize(X_train.copy(), X_test.copy())\n",
    "\n",
    "# svd = TruncatedSVD(n_components=324, n_iter=20, random_state=42)\n",
    "# svd.fit(X_train)\n",
    "# X_train = svd.transform(X_train)\n",
    "# X_test = svd.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, BatchNormalization, LocallyConnected1D\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_bags = 5\n",
    "kf = KFold(n_splits=n_bags)\n",
    "models = []\n",
    "\n",
    "for i in range(n_bags):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(X_train.shape[1],), kernel_regularizer=regularizers.l2(0.000)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Dense(128, kernel_regularizer=regularizers.l2(0.000)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Dense(128, kernel_regularizer=regularizers.l2(0.000)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43972 samples, validate on 10994 samples\n",
      "Epoch 1/10\n",
      "43972/43972 [==============================] - 6s - loss: 0.5193 - acc: 0.7489 - val_loss: 0.4928 - val_acc: 0.7669\n",
      "Epoch 2/10\n",
      "43972/43972 [==============================] - 3s - loss: 0.4915 - acc: 0.7656 - val_loss: 0.4894 - val_acc: 0.7656\n",
      "Epoch 3/10\n",
      "43972/43972 [==============================] - 3s - loss: 0.4827 - acc: 0.7695 - val_loss: 0.4840 - val_acc: 0.7728\n",
      "Epoch 4/10\n",
      "43972/43972 [==============================] - 3s - loss: 0.4779 - acc: 0.7727 - val_loss: 0.4824 - val_acc: 0.7713\n",
      "Epoch 5/10\n",
      "43972/43972 [==============================] - 3s - loss: 0.4743 - acc: 0.7762 - val_loss: 0.4808 - val_acc: 0.7701\n",
      "Epoch 6/10\n",
      "43972/43972 [==============================] - 3s - loss: 0.4711 - acc: 0.7775 - val_loss: 0.4819 - val_acc: 0.7691\n",
      "Epoch 7/10\n",
      "43972/43972 [==============================] - 3s - loss: 0.4696 - acc: 0.7795 - val_loss: 0.4804 - val_acc: 0.7707\n",
      "Epoch 8/10\n",
      "43972/43972 [==============================] - 3s - loss: 0.4675 - acc: 0.7801 - val_loss: 0.4868 - val_acc: 0.7697\n",
      "Epoch 9/10\n",
      "43972/43972 [==============================] - 3s - loss: 0.4649 - acc: 0.7816 - val_loss: 0.4820 - val_acc: 0.7721\n",
      "Epoch 10/10\n",
      "43972/43972 [==============================] - 4s - loss: 0.4641 - acc: 0.7830 - val_loss: 0.4810 - val_acc: 0.7747\n",
      "Train on 43973 samples, validate on 10993 samples\n",
      "Epoch 1/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.5192 - acc: 0.7515 - val_loss: 0.4968 - val_acc: 0.7670\n",
      "Epoch 2/10\n",
      "43973/43973 [==============================] - 3s - loss: 0.4917 - acc: 0.7645 - val_loss: 0.4860 - val_acc: 0.7701\n",
      "Epoch 3/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4821 - acc: 0.7697 - val_loss: 0.4841 - val_acc: 0.7708\n",
      "Epoch 4/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4777 - acc: 0.7745 - val_loss: 0.4832 - val_acc: 0.7707\n",
      "Epoch 5/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4733 - acc: 0.7759 - val_loss: 0.4801 - val_acc: 0.7745\n",
      "Epoch 6/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4721 - acc: 0.7771 - val_loss: 0.4787 - val_acc: 0.7731\n",
      "Epoch 7/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4703 - acc: 0.7785 - val_loss: 0.4805 - val_acc: 0.7756\n",
      "Epoch 8/10\n",
      "43973/43973 [==============================] - 3s - loss: 0.4666 - acc: 0.7815 - val_loss: 0.4787 - val_acc: 0.7724\n",
      "Epoch 9/10\n",
      "43973/43973 [==============================] - 3s - loss: 0.4653 - acc: 0.7837 - val_loss: 0.4776 - val_acc: 0.7750\n",
      "Epoch 10/10\n",
      "43973/43973 [==============================] - 3s - loss: 0.4622 - acc: 0.7833 - val_loss: 0.4822 - val_acc: 0.7709\n",
      "Train on 43973 samples, validate on 10993 samples\n",
      "Epoch 1/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.5243 - acc: 0.7483 - val_loss: 0.4874 - val_acc: 0.7679\n",
      "Epoch 2/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4915 - acc: 0.7661 - val_loss: 0.4790 - val_acc: 0.7739\n",
      "Epoch 3/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4830 - acc: 0.7697 - val_loss: 0.4786 - val_acc: 0.7764\n",
      "Epoch 4/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4788 - acc: 0.7714 - val_loss: 0.4799 - val_acc: 0.7736\n",
      "Epoch 5/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4754 - acc: 0.7753 - val_loss: 0.4828 - val_acc: 0.7763\n",
      "Epoch 6/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4728 - acc: 0.7764 - val_loss: 0.4771 - val_acc: 0.7767\n",
      "Epoch 7/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4689 - acc: 0.7781 - val_loss: 0.4848 - val_acc: 0.7753\n",
      "Epoch 8/10\n",
      "43973/43973 [==============================] - 3s - loss: 0.4684 - acc: 0.7815 - val_loss: 0.4757 - val_acc: 0.7772\n",
      "Epoch 9/10\n",
      "43973/43973 [==============================] - 3s - loss: 0.4657 - acc: 0.7813 - val_loss: 0.4771 - val_acc: 0.7799\n",
      "Epoch 10/10\n",
      "43973/43973 [==============================] - 3s - loss: 0.4628 - acc: 0.7821 - val_loss: 0.4779 - val_acc: 0.7789\n",
      "Train on 43973 samples, validate on 10993 samples\n",
      "Epoch 1/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.5200 - acc: 0.7514 - val_loss: 0.4817 - val_acc: 0.7653\n",
      "Epoch 2/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4908 - acc: 0.7654 - val_loss: 0.4775 - val_acc: 0.7735\n",
      "Epoch 3/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4819 - acc: 0.7714 - val_loss: 0.4769 - val_acc: 0.7709\n",
      "Epoch 4/10\n",
      "43973/43973 [==============================] - 3s - loss: 0.4784 - acc: 0.7742 - val_loss: 0.4836 - val_acc: 0.7712\n",
      "Epoch 5/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4756 - acc: 0.7761 - val_loss: 0.4777 - val_acc: 0.7716\n",
      "Epoch 6/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4732 - acc: 0.7777 - val_loss: 0.4738 - val_acc: 0.7719\n",
      "Epoch 7/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4698 - acc: 0.7818 - val_loss: 0.4748 - val_acc: 0.7752\n",
      "Epoch 8/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4686 - acc: 0.7813 - val_loss: 0.4726 - val_acc: 0.7774\n",
      "Epoch 9/10\n",
      "43973/43973 [==============================] - 3s - loss: 0.4655 - acc: 0.7842 - val_loss: 0.4741 - val_acc: 0.7770\n",
      "Epoch 10/10\n",
      "43973/43973 [==============================] - 3s - loss: 0.4629 - acc: 0.7846 - val_loss: 0.4743 - val_acc: 0.7726\n",
      "Train on 43973 samples, validate on 10993 samples\n",
      "Epoch 1/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.5224 - acc: 0.7503 - val_loss: 0.4893 - val_acc: 0.7662\n",
      "Epoch 2/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4911 - acc: 0.7658 - val_loss: 0.4822 - val_acc: 0.7679\n",
      "Epoch 3/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4827 - acc: 0.7704 - val_loss: 0.4829 - val_acc: 0.7698\n",
      "Epoch 4/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4771 - acc: 0.7737 - val_loss: 0.4779 - val_acc: 0.7723\n",
      "Epoch 5/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4748 - acc: 0.7765 - val_loss: 0.4804 - val_acc: 0.7698\n",
      "Epoch 6/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4718 - acc: 0.7772 - val_loss: 0.4824 - val_acc: 0.7729\n",
      "Epoch 7/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4697 - acc: 0.7806 - val_loss: 0.4773 - val_acc: 0.7731\n",
      "Epoch 8/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4669 - acc: 0.7817 - val_loss: 0.4781 - val_acc: 0.7740\n",
      "Epoch 9/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4645 - acc: 0.7841 - val_loss: 0.4765 - val_acc: 0.7762\n",
      "Epoch 10/10\n",
      "43973/43973 [==============================] - 4s - loss: 0.4624 - acc: 0.7828 - val_loss: 0.4789 - val_acc: 0.7741\n"
     ]
    }
   ],
   "source": [
    "i =0\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "    Y_train_fold, Y_test_fold = Y_train[train_index], Y_train[test_index]\n",
    "    models[i].compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # Train the model, iterating on the data in batches of 64 samples\n",
    "    history = models[i].fit(X_train_fold, Y_train_fold, epochs=10, batch_size=64,\n",
    "                    validation_data=(X_test_fold, Y_test_fold))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9056/9701 [===========================>..] - ETA: 0s0.0\n",
      "0.7746624059375322\n"
     ]
    }
   ],
   "source": [
    "probs = np.empty((5,X_test.shape[0], 2))\n",
    "i = 0\n",
    "for model in models:\n",
    "    probs[i] = model.predict_proba(X_test)\n",
    "    i += 1\n",
    "    \n",
    "probs_mean = np.mean(probs, axis = 0)\n",
    "rounded = np.round(probs_mean)\n",
    "errors = 0\n",
    "print(rounded[0][0])\n",
    "for i in range(rounded.shape[0]):\n",
    "    if (rounded[i][0] != Y_test[i][0]):\n",
    "        errors += 1\n",
    "\n",
    "acc = 1 - (errors / Y_test.shape[0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ADA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78517678589836104"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble of Random Forests, getting great test accuracy to train time ratio\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "#clf.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Starting ADA\")\n",
    "# Now, use this as the base classifier for the adaboost classifier\n",
    "clf1 = AdaBoostClassifier(base_estimator=clf, n_estimators=100)\n",
    "clf1.fit(X_train, Y_train_1d)\n",
    "\n",
    "clf1.score(X_test, Y_test_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74487166271518401"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Make a Adaboosted MLP classifier. Should be dank af.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clfMLP = MLPClassifier(hidden_layer_sizes=(128,128,128), early_stopping=True, learning_rate='adaptive')\n",
    "clfMLP.fit(X_train, Y_train)\n",
    "clfMLP.score(X_test, Y_test)\n",
    "\n",
    "# clfDANK = AdaBoostClassifier(base_estimator=clf, n_estimators=10)\n",
    "# clfDANK.fit(X_train, Y_train_1d)\n",
    "# clfDANK.score(X_test, Y_test_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM Classifier. This might be interesting to ADABoost. Takes a long time to train.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clfSVC = SVC()\n",
    "clfSVC.fit(X_test, Y_test_1d) # Train with smaller dataset to cuts down time.\n",
    "clfSVC.score(X_train, Y_train_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
