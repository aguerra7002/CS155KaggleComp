{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Seed the random number generator:\n",
    "np.random.seed(1)\n",
    "\n",
    "def load_data(filename, skiprows = 1):\n",
    "    \"\"\"\n",
    "    Function loads data stored in the file filename and returns it as a numpy ndarray.\n",
    "    \n",
    "    Inputs:\n",
    "        filename: given as a string.\n",
    "        \n",
    "    Outputs:\n",
    "        Data contained in the file, returned as a numpy ndarray\n",
    "    \"\"\"\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load all the data\n",
    "all = load_data('train_2008.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.0]\n",
      "#1: (64667, 1)\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 12.0, 14.0]\n",
      "#1: (64667, 12)\n",
      "[1.0, 2.0, 4.0, 201.0, 203.0]\n",
      "#1: (64667, 5)\n",
      "[0.0, 1.0]\n",
      "#1: (64667, 2)\n",
      "[1.0, 2.0, 3.0]\n",
      "#1: (64667, 3)\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 10.0, 11.0, 12.0]\n",
      "#1: (64667, 11)\n",
      "[1.0, 2.0]\n",
      "#1: (64667, 2)\n",
      "[-3.0, -2.0, -1.0, 1.0, 2.0]\n",
      "#1: (64667, 5)\n",
      "[-1.0, 0.0, 1.0]\n",
      "#1: (64667, 3)\n",
      "[-3.0, -2.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0]\n",
      "#1: (64667, 18)\n",
      "[-1.0, 1.0, 2.0]\n",
      "#1: (64667, 3)\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 14.0, 16.0]\n",
      "#1: (64667, 14)\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
      "#1: (64667, 10)\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]\n",
      "#1: (64667, 8)\n",
      "[-1.0, 1.0, 2.0]\n",
      "#1: (64667, 3)\n",
      "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
      "#1: (64667, 10)\n",
      "[0.0, 2.0, 3.0]\n",
      "#1: (64667, 3)\n",
      "[83001.0, 83002.0, 83003.0, 83004.0, 83006.0, 83011.0, 83021.0, 83031.0, 83041.0, 83051.0, 83101.0, 83111.0, 83131.0, 83141.0, 83201.0, 83241.0, 83251.0, 83261.0, 83262.0, 85001.0, 85002.0, 85003.0, 85011.0, 85021.0, 85031.0, 85041.0, 85231.0, 85241.0, 85251.0, 85261.0]\n",
      "#1: (64667, 30)\n",
      "[-3.0, -2.0, 1.0, 2.0]\n",
      "#1: (64667, 4)\n",
      "[-3.0, -2.0, -1.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "#1: (64667, 10)\n",
      "[-1.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "#1: (64667, 7)\n",
      "[-1.0, 3.0, 4.0]\n",
      "#1: (64667, 3)\n",
      "[-1.0, 4.0]\n",
      "#1: (64667, 2)\n",
      "[1.0, 2.0, 3.0, 4.0]\n",
      "#1: (64667, 4)\n",
      "[11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 21.0, 22.0, 23.0, 31.0, 32.0, 33.0, 34.0, 35.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 51.0, 52.0, 53.0, 54.0, 55.0, 56.0, 57.0, 58.0, 59.0, 61.0, 62.0, 63.0, 64.0, 71.0, 72.0, 73.0, 74.0, 81.0, 82.0, 83.0, 84.0, 85.0, 86.0, 87.0, 88.0, 91.0, 92.0, 93.0, 94.0, 95.0]\n",
      "#1: (64667, 51)\n",
      "[1.0, 2.0, 4.0, 5.0, 6.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 53.0, 54.0, 55.0, 56.0]\n",
      "#1: (64667, 51)\n",
      "[1.0, 2.0, 3.0, 4.0]\n",
      "#1: (64667, 4)\n",
      "[1.0, 2.0, 3.0]\n",
      "#1: (64667, 3)\n",
      "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "#1: (64667, 8)\n",
      "[0.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "#1: (64667, 7)\n",
      "[0.0, 118.0, 176.0, 184.0, 212.0, 216.0, 220.0, 260.0, 266.0, 268.0, 272.0, 290.0, 294.0, 304.0, 348.0, 356.0, 376.0, 378.0, 408.0, 428.0, 450.0, 482.0, 488.0, 500.0, 548.0, 715.0, 720.0]\n",
      "#1: (64667, 27)\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0]\n",
      "#1: (64667, 17)\n",
      "[-1.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0]\n",
      "#1: (64667, 12)\n",
      "[0.0, 1.0]\n",
      "#1: (64667, 2)\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0]\n",
      "#1: (64667, 6)\n",
      "[-1.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 12.0, 13.0, 15.0]\n",
      "#1: (64667, 14)\n",
      "[1.0, 2.0]\n",
      "#1: (64667, 2)\n",
      "[1.0, 2.0]\n",
      "#1: (64667, 2)\n",
      "[31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0]\n",
      "#1: (64667, 16)\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 20.0, 21.0]\n",
      "#1: (64667, 20)\n",
      "[-1.0, 1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "#1: (64667, 6)\n",
      "[1.0, 2.0, 3.0, 5.0, 9.0]\n",
      "#1: (64667, 5)\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0]\n",
      "#1: (64667, 14)\n",
      "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "#1: (64667, 6)\n",
      "[0.0, 1.0, 2.0, 3.0, 4.0]\n",
      "#1: (64667, 5)\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "#1: (64667, 5)\n",
      "[1.0, 2.0]\n",
      "#1: (64667, 2)\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0]\n",
      "#1: (64667, 7)\n",
      "[57.0, 66.0, 73.0, 78.0, 96.0, 100.0, 102.0, 103.0, 104.0, 105.0, 106.0, 108.0, 109.0, 110.0, 116.0, 117.0, 119.0, 120.0, 126.0, 127.0, 128.0, 129.0, 130.0, 132.0, 134.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 154.0, 156.0, 157.0, 158.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 200.0, 202.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 220.0, 222.0, 223.0, 224.0, 226.0, 231.0, 233.0, 235.0, 236.0, 238.0, 239.0, 240.0, 242.0, 243.0, 246.0, 247.0, 248.0, 249.0, 300.0, 301.0, 303.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 321.0, 323.0, 324.0, 327.0, 328.0, 329.0, 330.0, 332.0, 333.0, 339.0, 340.0, 341.0, 343.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 368.0, 369.0, 370.0, 372.0, 373.0, 374.0, 399.0, 407.0, 408.0, 414.0, 416.0, 421.0, 427.0, 429.0, 436.0, 440.0, 447.0, 448.0, 449.0, 451.0, 453.0, 457.0, 461.0, 462.0, 501.0, 508.0, 515.0, 523.0, 527.0, 528.0, 555.0]\n",
      "#1: (64667, 143)\n",
      "[57.0, 66.0, 73.0, 78.0, 96.0, 100.0, 102.0, 103.0, 104.0, 105.0, 106.0, 108.0, 109.0, 110.0, 116.0, 117.0, 119.0, 120.0, 126.0, 127.0, 128.0, 129.0, 130.0, 132.0, 134.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 154.0, 156.0, 157.0, 158.0, 159.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 200.0, 202.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 220.0, 222.0, 223.0, 224.0, 226.0, 231.0, 233.0, 235.0, 236.0, 238.0, 239.0, 240.0, 242.0, 243.0, 246.0, 247.0, 248.0, 249.0, 300.0, 301.0, 303.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 321.0, 323.0, 324.0, 327.0, 328.0, 329.0, 330.0, 332.0, 333.0, 339.0, 340.0, 341.0, 343.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 368.0, 369.0, 370.0, 372.0, 373.0, 374.0, 399.0, 407.0, 408.0, 414.0, 416.0, 421.0, 427.0, 429.0, 436.0, 440.0, 444.0, 447.0, 448.0, 449.0, 451.0, 453.0, 457.0, 461.0, 462.0, 501.0, 508.0, 515.0, 523.0, 527.0, 528.0, 555.0]\n",
      "#1: (64667, 145)\n",
      "[57.0, 66.0, 73.0, 78.0, 96.0, 100.0, 102.0, 103.0, 104.0, 105.0, 106.0, 108.0, 109.0, 110.0, 116.0, 117.0, 119.0, 120.0, 126.0, 127.0, 128.0, 129.0, 130.0, 132.0, 134.0, 136.0, 137.0, 138.0, 139.0, 140.0, 141.0, 142.0, 147.0, 148.0, 149.0, 150.0, 151.0, 152.0, 154.0, 156.0, 157.0, 158.0, 160.0, 161.0, 162.0, 163.0, 164.0, 165.0, 166.0, 167.0, 200.0, 202.0, 205.0, 206.0, 207.0, 208.0, 209.0, 210.0, 211.0, 212.0, 213.0, 214.0, 215.0, 216.0, 217.0, 220.0, 222.0, 223.0, 224.0, 226.0, 231.0, 233.0, 235.0, 238.0, 239.0, 240.0, 242.0, 243.0, 247.0, 248.0, 249.0, 300.0, 301.0, 303.0, 310.0, 311.0, 312.0, 313.0, 314.0, 315.0, 316.0, 321.0, 323.0, 324.0, 327.0, 328.0, 329.0, 330.0, 332.0, 333.0, 338.0, 340.0, 341.0, 343.0, 360.0, 361.0, 362.0, 363.0, 364.0, 365.0, 368.0, 369.0, 370.0, 372.0, 373.0, 374.0, 399.0, 400.0, 407.0, 408.0, 414.0, 416.0, 417.0, 421.0, 427.0, 429.0, 436.0, 440.0, 447.0, 448.0, 449.0, 451.0, 453.0, 457.0, 461.0, 462.0, 501.0, 508.0, 515.0, 523.0, 527.0, 528.0, 555.0]\n",
      "#1: (64667, 143)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-431e54db9046>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# print(test.shape, newX.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mnewX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5145\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5146\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5147\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####### FEATURE ENGINEERING HERE #######\n",
    "# Below, we transform the real valued \n",
    "# data from the dataset to categorical \n",
    "# data if it should be. This should \n",
    "# vastly improve model performance.\n",
    "########################################\n",
    "import csv\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Split into features and labels\n",
    "Y = all[:, -1]\n",
    "X = all[:, :-1]\n",
    "\n",
    "featureNames = None\n",
    "with open('train_2008.csv', 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        featureNames = row\n",
    "        break\n",
    "\n",
    "# Delete all the columns which are identical among all data points, these only hurt our model \n",
    "shouldDelArr = [\"id\", \"QSTNUM\"]\n",
    "shouldDelete = np.all(X == X[0,:], axis = 0)\n",
    "for i in range(X.shape[1]):\n",
    "    ib = X.shape[1] - i - 1\n",
    "    if ((shouldDelete[ib] == True) or (featureNames[ib] in shouldDelArr)):\n",
    "        # delete the i-th columnif \n",
    "        X = np.delete(X, ib, 1)\n",
    "        featureNames = np.delete(featureNames, ib)\n",
    "\n",
    "newX = np.empty((X.shape[0],1))\n",
    "\n",
    "# These are the features which are NOT categorical; the rest I deemed were.\n",
    "real_valued_feats = [\"HWHHWGT\", \"GTCBSA\", \"GTCO\", \"PEAGE\", \"PEHRUSL1\", \"PEHRUSL2\", \"PUHROFF2\", \"PUHROT2\", \"PEHRACT2\", \n",
    "                     \"PELAYDUR\", \"PELKDUR\", \"PRUNEDUR\", \"PEERNHRO\", \"PEERNWKP\", \"PRNMCHLD\", \"PEHGCOMP\", \"PECYC\", \"PWCMPWGT\"]\n",
    "\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    feature = np.reshape(X[:, i], (X.shape[0], 1))\n",
    "    \n",
    "    num_distinct_negs = 0\n",
    "    min_neg = 0\n",
    "    distinct_negs = []\n",
    "    for val in feature:\n",
    "        if val < 0:\n",
    "            if val not in distinct_negs:\n",
    "                distinct_negs.append(val)\n",
    "                num_distinct_negs += 1\n",
    "            if val < min_neg:\n",
    "                min_neg = val\n",
    "    \n",
    "    \n",
    "    #print(feature.shape)\n",
    "    if featureNames[i] in real_valued_feats:\n",
    "        newX = np.append(newX, feature, axis = 1)\n",
    "    else:\n",
    "        distinct_negs = []\n",
    "        distinct_nums = []\n",
    "        for val in feature:\n",
    "            if val[0] < 0:\n",
    "                if val[0] not in distinct_negs:\n",
    "                    distinct_negs.append(val[0])\n",
    "            if val[0] not in distinct_nums:\n",
    "                distinct_nums.append(val[0])\n",
    "        \n",
    "        distinct_nums.sort()\n",
    "        print(distinct_nums)\n",
    "        for i in range(feature.shape[0]):\n",
    "            feature[i][0] = distinct_nums.index(feature[i][0])\n",
    "        \n",
    "        feature = keras.utils.np_utils.to_categorical(feature)\n",
    "        print(\"#1:\", cat.shape)\n",
    "#         # delete the zero columns\n",
    "#         zero_cols = np.any(cat, axis=0)\n",
    "#         print\n",
    "#         for j in range(zero_cols.shape[0]):\n",
    "#             jb = zero_cols.shape[0] - j - 1\n",
    "#             if zero_cols[jb] == False:\n",
    "#                 cat = np.delete(cat, jb, axis=1)     \n",
    "        # Delete the negative cols\n",
    "        feature = np.delete(feature, np.s_[0:len(distinct_negs)], 1) \n",
    "        \n",
    "        # print(test.shape, newX.shape)\n",
    "        newX = np.append(newX, feature, axis = 1)\n",
    "        \n",
    "        \n",
    "# Delete the placeholder column and transpose\n",
    "newX = np.delete(newX, 0, 1)\n",
    "print(newX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into train and validation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(rows_train, rows_test):\n",
    "    columns_train  = np.transpose(rows_train)\n",
    "    columns_test = np.transpose(rows_test)\n",
    "    for i in range(columns_train.shape[0]):\n",
    "        col = columns_train[i]\n",
    "        mean = np.mean(col)\n",
    "        stddev = np.std(col)\n",
    "        if (stddev == 0):\n",
    "            stddev = 1\n",
    "        columns_train[i] = (col - mean) / stddev\n",
    "        columns_test[i] = (columns_test[i] - mean) / stddev\n",
    "    return np.transpose(columns_train), np.transpose(columns_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_1d = Y_train.copy()\n",
    "Y_test_1d = Y_test.copy()\n",
    "\n",
    "# First convert to real labels\n",
    "Y_train = keras.utils.np_utils.to_categorical(Y_train)\n",
    "Y_test = keras.utils.np_utils.to_categorical(Y_test)\n",
    "\n",
    "\n",
    "X_train, X_test = normalize(X_train.copy(), X_test.copy())\n",
    "\n",
    "# svd = TruncatedSVD(n_components=324, n_iter=20, random_state=42)\n",
    "# svd.fit(X_train)\n",
    "# X_train = svd.transform(X_train)\n",
    "# X_test = svd.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, BatchNormalization, LocallyConnected1D\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_bags = 2\n",
    "kf = KFold(n_splits=n_bags)\n",
    "models = []\n",
    "\n",
    "for i in range(n_bags):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(X_train.shape[1],), kernel_regularizer=regularizers.l2(0.000)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Dense(128, kernel_regularizer=regularizers.l2(0.000)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Dense(128, kernel_regularizer=regularizers.l2(0.000)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27483 samples, validate on 27483 samples\n",
      "Epoch 1/10\n",
      "27483/27483 [==============================] - 5s - loss: 0.5270 - acc: 0.7445 - val_loss: 0.4935 - val_acc: 0.7613\n",
      "Epoch 2/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4933 - acc: 0.7623 - val_loss: 0.4867 - val_acc: 0.7676\n",
      "Epoch 3/10\n",
      "27483/27483 [==============================] - 2s - loss: 0.4845 - acc: 0.7672 - val_loss: 0.4896 - val_acc: 0.7712\n",
      "Epoch 4/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4761 - acc: 0.7727 - val_loss: 0.4848 - val_acc: 0.7701\n",
      "Epoch 5/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4743 - acc: 0.7734 - val_loss: 0.4877 - val_acc: 0.7724\n",
      "Epoch 6/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4688 - acc: 0.7780 - val_loss: 0.4877 - val_acc: 0.7704\n",
      "Epoch 7/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4655 - acc: 0.7804 - val_loss: 0.4835 - val_acc: 0.7710\n",
      "Epoch 8/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4627 - acc: 0.7818 - val_loss: 0.4866 - val_acc: 0.7719\n",
      "Epoch 9/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4600 - acc: 0.7822 - val_loss: 0.4849 - val_acc: 0.7736\n",
      "Epoch 10/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4565 - acc: 0.7865 - val_loss: 0.4835 - val_acc: 0.7739\n",
      "Train on 27483 samples, validate on 27483 samples\n",
      "Epoch 1/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.5307 - acc: 0.7473 - val_loss: 0.4909 - val_acc: 0.7631\n",
      "Epoch 2/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4958 - acc: 0.7619 - val_loss: 0.4841 - val_acc: 0.7677\n",
      "Epoch 3/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4847 - acc: 0.7690 - val_loss: 0.4881 - val_acc: 0.7662\n",
      "Epoch 4/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4798 - acc: 0.7742 - val_loss: 0.4873 - val_acc: 0.7679\n",
      "Epoch 5/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4754 - acc: 0.7771 - val_loss: 0.4827 - val_acc: 0.7701\n",
      "Epoch 6/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4722 - acc: 0.7786 - val_loss: 0.4804 - val_acc: 0.7716\n",
      "Epoch 7/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4686 - acc: 0.7818 - val_loss: 0.4837 - val_acc: 0.7698\n",
      "Epoch 8/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4665 - acc: 0.7817 - val_loss: 0.4851 - val_acc: 0.7708\n",
      "Epoch 9/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4636 - acc: 0.7841 - val_loss: 0.4836 - val_acc: 0.7719\n",
      "Epoch 10/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4614 - acc: 0.7840 - val_loss: 0.4809 - val_acc: 0.7719\n"
     ]
    }
   ],
   "source": [
    "i =0\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "    Y_train_fold, Y_test_fold = Y_train[train_index], Y_train[test_index]\n",
    "    models[i].compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # Train the model, iterating on the data in batches of 64 samples\n",
    "    history = models[i].fit(X_train_fold, Y_train_fold, epochs=10, batch_size=64,\n",
    "                    validation_data=(X_test_fold, Y_test_fold))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9056/9701 [===========================>..] - ETA: 0s0.0\n",
      "0.7746624059375322\n"
     ]
    }
   ],
   "source": [
    "probs = np.empty((5,X_test.shape[0], 2))\n",
    "i = 0\n",
    "for model in models:\n",
    "    probs[i] = model.predict_proba(X_test)\n",
    "    i += 1\n",
    "    \n",
    "probs_mean = np.mean(probs, axis = 0)\n",
    "rounded = np.round(probs_mean)\n",
    "errors = 0\n",
    "print(rounded[0][0])\n",
    "for i in range(rounded.shape[0]):\n",
    "    if (rounded[i][0] != Y_test[i][0]):\n",
    "        errors += 1\n",
    "\n",
    "acc = 1 - (errors / Y_test.shape[0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ADA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78517678589836104"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble of Random Forests, getting great test accuracy to train time ratio\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "#clf.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Starting ADA\")\n",
    "# Now, use this as the base classifier for the adaboost classifier\n",
    "clf1 = AdaBoostClassifier(base_estimator=clf, n_estimators=100)\n",
    "clf1.fit(X_train, Y_train_1d)\n",
    "\n",
    "clf1.score(X_test, Y_test_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74487166271518401"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Make a Adaboosted MLP classifier. Should be dank af.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clfMLP = MLPClassifier(hidden_layer_sizes=(128,128,128), early_stopping=True, learning_rate='adaptive')\n",
    "clfMLP.fit(X_train, Y_train)\n",
    "clfMLP.score(X_test, Y_test)\n",
    "\n",
    "# clfDANK = AdaBoostClassifier(base_estimator=clf, n_estimators=10)\n",
    "# clfDANK.fit(X_train, Y_train_1d)\n",
    "# clfDANK.score(X_test, Y_test_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM Classifier. This might be interesting to ADABoost. Takes a long time to train.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clfSVC = SVC()\n",
    "clfSVC.fit(X_test, Y_test_1d) # Train with smaller dataset to cuts down time.\n",
    "clfSVC.score(X_train, Y_train_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
