{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Seed the random number generator:\n",
    "np.random.seed(1)\n",
    "\n",
    "def load_data(filename, skiprows = 1):\n",
    "    \"\"\"\n",
    "    Function loads data stored in the file filename and returns it as a numpy ndarray.\n",
    "    \n",
    "    Inputs:\n",
    "        filename: given as a string.\n",
    "        \n",
    "    Outputs:\n",
    "        Data contained in the file, returned as a numpy ndarray\n",
    "    \"\"\"\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the data\n",
    "all = load_data('train_2008.csv')\n",
    "all2 = load_data('test_2008.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 HRHHID2\n",
      "51 GESTCEN\n",
      "51 GESTFIPS\n",
      "27 GTCSA\n",
      "32 PRABSREA\n",
      "53 PRDTIND1\n",
      "41 PRDTIND2\n",
      "262 PEIO1ICD\n",
      "123 PEIO2ICD\n",
      "(64667, 2419)\n",
      "(16001, 2419)\n"
     ]
    }
   ],
   "source": [
    "####### FEATURE ENGINEERING HERE #######\n",
    "# Below, we transform the real valued \n",
    "# data from the dataset to categorical \n",
    "# data if it should be. This should \n",
    "# vastly improve model performance.\n",
    "########################################\n",
    "import csv\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Split into features and labels\n",
    "Y = all[:, -1]\n",
    "X = all[:, :-1]\n",
    "comp1X = all2[:, :]\n",
    "comp1X = np.append(comp1X, [all2[1, :]], axis = 0)\n",
    "\n",
    "featureNames = None\n",
    "with open('train_2008.csv', 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        featureNames = row\n",
    "        break\n",
    "\n",
    "# Delete all the columns which are identical among all data points, these only hurt our model \n",
    "shouldDelArr = [\"id\", \"QSTNUM\", \"PEIO1OCD\", \"PEIO2OCD\", \"PEHRACTT\", \"PEHRUSLT\"]\n",
    "shouldDelete = np.all(X == X[0,:], axis = 0)\n",
    "for i in range(X.shape[1]):\n",
    "    ib = X.shape[1] - i - 1\n",
    "    if ((shouldDelete[ib] == True) or (featureNames[ib] in shouldDelArr)):\n",
    "        # delete the i-th columnif \n",
    "        X = np.delete(X, ib, 1)\n",
    "        comp1X = np.delete(comp1X, ib, 1)\n",
    "        featureNames = np.delete(featureNames, ib)\n",
    "\n",
    "newX = np.empty((X.shape[0],1))\n",
    "newc1X = np.empty((comp1X.shape[0],1))\n",
    "\n",
    "# These are the features which are NOT categorical; the rest I deemed were.\n",
    "real_valued_feats = [\"HWHHWGT\", \"GTCBSA\", \"GTCO\", \"PEAGE\", \"PEHRUSL1\", \"PEHRUSL2\", \"PUHROFF2\", \"PUHROT2\", \"PEHRACT2\", \n",
    "                     \"PELAYDUR\", \"PELKDUR\", \"PRUNEDUR\", \"PEERNHRO\", \"PEERNWKP\", \"PRNMCHLD\", \"PEHGCOMP\", \"PECYC\", \"PWCMPWGT\", \n",
    "                     \"PWFMWGT\", \"PWLGWGT\", \"PWORWGT\", \"PWSSWGT\", \"PWVETWGT\", \"PRERNWA\", \"PRERNHLY\", \"PEERNH1O\", \"PEERNH2\", \n",
    "                     \"PEERN\", \"PEHRACT1\"]\n",
    "\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    feature = np.reshape(X[:, i], (X.shape[0], 1))\n",
    "    featurec1 = np.reshape(comp1X[:, i], (comp1X.shape[0], 1))\n",
    "    if featureNames[i] in [\"PENATVTY\", \"PEMNTVTY\", \"PEFNTVTY\"]:\n",
    "        feature[feature == 55] = 1 # US\n",
    "        feature[feature == 66] = 2 # Guam\n",
    "        feature[feature == 73] = 3 # Puerto Rico\n",
    "        feature[feature == 78] = 4 # US Virgin Islands\n",
    "        feature[feature == 96] = 5 # Other US Island Area\n",
    "        feature[feature > 5] = 6\n",
    "        \n",
    "        featurec1[featurec1 == 55] = 1 # US\n",
    "        featurec1[featurec1 == 66] = 2 # Guam\n",
    "        featurec1[featurec1 == 73] = 3 # Puerto Rico\n",
    "        featurec1[featurec1 == 78] = 4 # US Virgin Islands\n",
    "        featurec1[featurec1 == 96] = 5 # Other US Island Area\n",
    "        featurec1[featurec1 > 5] = 6 \n",
    "            \n",
    "    \n",
    "    #print(feature.shape)\n",
    "    if featureNames[i] in real_valued_feats:\n",
    "        newX = np.append(newX, feature, axis = 1)\n",
    "        newc1X = np.append(newc1X, featurec1, axis = 1)\n",
    "    else:\n",
    "        distinct_negs = []\n",
    "        distinct_nums = []\n",
    "        for val in feature:\n",
    "            if val[0] < 0:\n",
    "                if val[0] not in distinct_negs:\n",
    "                    distinct_negs.append(val[0])\n",
    "            if val[0] not in distinct_nums:\n",
    "                distinct_nums.append(val[0])\n",
    "        \n",
    "        distinct_nums.sort()\n",
    "        if (len(distinct_nums)> 25):\n",
    "            print(len(distinct_nums), featureNames[i])\n",
    "            \n",
    "        for i in range(feature.shape[0]):\n",
    "            feature[i][0] = distinct_nums.index(feature[i][0])\n",
    "        \n",
    "        for i in range(featurec1.shape[0]):\n",
    "            if (featurec1[i][0] not in distinct_nums):\n",
    "                featurec1[i][0] = feature[i][0]\n",
    "            else:\n",
    "                featurec1[i][0] = distinct_nums.index(featurec1[i][0])\n",
    "                \n",
    "            if (i == (featurec1.shape[0] - 1)):\n",
    "                featurec1[i][0]= len(distinct_nums) - 1\n",
    "        \n",
    "        feature = keras.utils.np_utils.to_categorical(feature)\n",
    "        featurec1 = keras.utils.np_utils.to_categorical(featurec1)\n",
    "   \n",
    "        # Delete the negative cols\n",
    "        feature = np.delete(feature, np.s_[0:len(distinct_negs)], 1)\n",
    "        featurec1 = np.delete(featurec1, np.s_[0:len(distinct_negs)], 1)\n",
    "        \n",
    "        # print(test.shape, newX.shape)\n",
    "        newX = np.append(newX, feature, axis = 1)\n",
    "        newc1X = np.append(newc1X, featurec1, axis = 1)\n",
    "        \n",
    "        \n",
    "# Delete the placeholder column and transpose\n",
    "newX = np.delete(newX, 0, 1)\n",
    "newc1X = np.delete(newc1X, 0, 1)\n",
    "print(newX.shape)\n",
    "print(newc1X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into train and validation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(newX, Y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(rows_train, rows_test):\n",
    "    columns_train  = np.transpose(rows_train)\n",
    "    columns_test = np.transpose(rows_test)\n",
    "    for i in range(columns_train.shape[0]):\n",
    "        col = columns_train[i]\n",
    "        mean = np.mean(col)\n",
    "        stddev = np.std(col)\n",
    "        if (stddev == 0):\n",
    "            stddev = 1\n",
    "        columns_train[i] = (col - mean) / stddev\n",
    "        columns_test[i] = (columns_test[i] - mean) / stddev\n",
    "    return np.transpose(columns_train), np.transpose(columns_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_1d = Y_train.copy()\n",
    "Y_test_1d = Y_test.copy()\n",
    "\n",
    "# First convert to real labels\n",
    "Y_train = keras.utils.np_utils.to_categorical(Y_train)\n",
    "Y_test = keras.utils.np_utils.to_categorical(Y_test)\n",
    "\n",
    "c1X_train, c1X_test = normalize(X_train.copy(), newc1X.copy())\n",
    "X_train, X_test = normalize(X_train.copy(), X_test.copy())\n",
    "\n",
    "# svd = TruncatedSVD(n_components=324, n_iter=20, random_state=42)\n",
    "# svd.fit(X_train)\n",
    "# X_train = svd.transform(X_train)\n",
    "# X_test = svd.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, BatchNormalization, LocallyConnected1D\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_bags = 2\n",
    "kf = KFold(n_splits=n_bags)\n",
    "models = []\n",
    "\n",
    "for i in range(n_bags):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2048, input_shape=(X_train.shape[1],), kernel_regularizer=regularizers.l2(0.0005)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(Dense(1024, kernel_regularizer=regularizers.l2(0.0005)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(Dense(512, kernel_regularizer=regularizers.l2(0.0005)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(Dense(128, kernel_regularizer=regularizers.l2(0.0005)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27483 samples, validate on 27483 samples\n",
      "Epoch 1/25\n",
      "27483/27483 [==============================] - 35s - loss: 1.7668 - acc: 0.7481 - val_loss: 1.2138 - val_acc: 0.7678\n",
      "Epoch 2/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.9388 - acc: 0.7705 - val_loss: 0.7730 - val_acc: 0.7538\n",
      "Epoch 3/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.6837 - acc: 0.7744 - val_loss: 0.6425 - val_acc: 0.7675\n",
      "Epoch 4/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.6031 - acc: 0.7743 - val_loss: 0.5966 - val_acc: 0.7660\n",
      "Epoch 5/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5762 - acc: 0.7786 - val_loss: 0.5834 - val_acc: 0.7717\n",
      "Epoch 6/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5671 - acc: 0.7775 - val_loss: 0.5817 - val_acc: 0.7711\n",
      "Epoch 7/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5642 - acc: 0.7816 - val_loss: 0.5912 - val_acc: 0.7627\n",
      "Epoch 8/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5624 - acc: 0.7832 - val_loss: 0.5770 - val_acc: 0.7751\n",
      "Epoch 9/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5563 - acc: 0.7845 - val_loss: 0.5744 - val_acc: 0.7730\n",
      "Epoch 10/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5498 - acc: 0.7856 - val_loss: 0.5768 - val_acc: 0.7747\n",
      "Epoch 11/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5430 - acc: 0.7867 - val_loss: 0.5721 - val_acc: 0.7740\n",
      "Epoch 12/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5339 - acc: 0.7898 - val_loss: 0.5674 - val_acc: 0.7716\n",
      "Epoch 13/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5286 - acc: 0.7910 - val_loss: 0.5622 - val_acc: 0.7721\n",
      "Epoch 14/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5218 - acc: 0.7952 - val_loss: 0.5624 - val_acc: 0.7731\n",
      "Epoch 15/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5171 - acc: 0.7945 - val_loss: 0.5628 - val_acc: 0.7713\n",
      "Epoch 16/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5164 - acc: 0.7941 - val_loss: 0.5641 - val_acc: 0.7753\n",
      "Epoch 17/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5106 - acc: 0.7964 - val_loss: 0.5637 - val_acc: 0.7681\n",
      "Epoch 18/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5044 - acc: 0.7982 - val_loss: 0.5663 - val_acc: 0.7610\n",
      "Epoch 19/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5045 - acc: 0.7981 - val_loss: 0.5610 - val_acc: 0.7670\n",
      "Epoch 20/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5013 - acc: 0.8027 - val_loss: 0.5682 - val_acc: 0.7689\n",
      "Epoch 21/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.4999 - acc: 0.8020 - val_loss: 0.5712 - val_acc: 0.7697\n",
      "Epoch 22/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.4974 - acc: 0.8032 - val_loss: 0.5680 - val_acc: 0.7677\n",
      "Epoch 23/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.4955 - acc: 0.8054 - val_loss: 0.5757 - val_acc: 0.7611\n",
      "Epoch 24/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.4947 - acc: 0.8069 - val_loss: 0.5699 - val_acc: 0.7699\n",
      "Epoch 25/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.4918 - acc: 0.8132 - val_loss: 0.5806 - val_acc: 0.7550\n",
      "Train on 27483 samples, validate on 27483 samples\n",
      "Epoch 1/25\n",
      "27483/27483 [==============================] - 35s - loss: 1.7374 - acc: 0.7514 - val_loss: 1.1788 - val_acc: 0.7683\n",
      "Epoch 2/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.9183 - acc: 0.7739 - val_loss: 0.7578 - val_acc: 0.7660\n",
      "Epoch 3/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.6759 - acc: 0.7765 - val_loss: 0.6376 - val_acc: 0.7661\n",
      "Epoch 4/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.6046 - acc: 0.7771 - val_loss: 0.5930 - val_acc: 0.7674\n",
      "Epoch 5/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5765 - acc: 0.7808 - val_loss: 0.5810 - val_acc: 0.7739\n",
      "Epoch 6/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5697 - acc: 0.7816 - val_loss: 0.5786 - val_acc: 0.7707\n",
      "Epoch 7/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5694 - acc: 0.7818 - val_loss: 0.5878 - val_acc: 0.7704\n",
      "Epoch 8/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5652 - acc: 0.7834 - val_loss: 0.5780 - val_acc: 0.7730\n",
      "Epoch 9/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5595 - acc: 0.7848 - val_loss: 0.5744 - val_acc: 0.7711\n",
      "Epoch 10/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5471 - acc: 0.7886 - val_loss: 0.5688 - val_acc: 0.7695\n",
      "Epoch 11/25\n",
      "27483/27483 [==============================] - 39s - loss: 0.5414 - acc: 0.7901 - val_loss: 0.5706 - val_acc: 0.7703\n",
      "Epoch 12/25\n",
      "27483/27483 [==============================] - 40s - loss: 0.5372 - acc: 0.7892 - val_loss: 0.5624 - val_acc: 0.7707\n",
      "Epoch 13/25\n",
      "27483/27483 [==============================] - 35s - loss: 0.5279 - acc: 0.7928 - val_loss: 0.5614 - val_acc: 0.7696\n",
      "Epoch 14/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5235 - acc: 0.7907 - val_loss: 0.5622 - val_acc: 0.7697\n",
      "Epoch 15/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5192 - acc: 0.7982 - val_loss: 0.5628 - val_acc: 0.7691\n",
      "Epoch 16/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5171 - acc: 0.8000 - val_loss: 0.5603 - val_acc: 0.7712\n",
      "Epoch 17/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5112 - acc: 0.7999 - val_loss: 0.5579 - val_acc: 0.7701\n",
      "Epoch 18/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5107 - acc: 0.7998 - val_loss: 0.5673 - val_acc: 0.7707\n",
      "Epoch 19/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5070 - acc: 0.8024 - val_loss: 0.5718 - val_acc: 0.7664\n",
      "Epoch 20/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5069 - acc: 0.8039 - val_loss: 0.5726 - val_acc: 0.7695\n",
      "Epoch 21/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.4999 - acc: 0.8052 - val_loss: 0.5670 - val_acc: 0.7697\n",
      "Epoch 22/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.5002 - acc: 0.8082 - val_loss: 0.5714 - val_acc: 0.7695\n",
      "Epoch 23/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.4977 - acc: 0.8120 - val_loss: 0.5692 - val_acc: 0.7693\n",
      "Epoch 24/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.4983 - acc: 0.8114 - val_loss: 0.5714 - val_acc: 0.7680\n",
      "Epoch 25/25\n",
      "27483/27483 [==============================] - 34s - loss: 0.4944 - acc: 0.8114 - val_loss: 0.5763 - val_acc: 0.7648\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "    Y_train_fold, Y_test_fold = Y_train[train_index], Y_train[test_index]\n",
    "    models[i].compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # Train the model, iterating on the data in batches of 64 samples\n",
    "    history = models[i].fit(X_train_fold, Y_train_fold, epochs=25, batch_size=64,\n",
    "                    validation_data=(X_test_fold, Y_test_fold))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9376/9701 [===========================>..] - ETA: 0s0.0\n",
      "0.2395629316565303\n"
     ]
    }
   ],
   "source": [
    "probs = np.empty((n_bags, c1X_test.shape[0], 2))\n",
    "i = 0\n",
    "for model in models:\n",
    "    probs[i] = model.predict_proba(c1X_test)\n",
    "    i+=1\n",
    "\n",
    "probs_meanc1 = np.mean(probs, axis = 0)\n",
    "\n",
    "probs = np.empty((5,X_test.shape[0], 2))\n",
    "i = 0\n",
    "for model in models:\n",
    "    probs[i] = model.predict_proba(X_test)\n",
    "    i += 1\n",
    "    \n",
    "probs_mean = np.mean(probs, axis = 0)\n",
    "rounded = np.round(probs_mean)\n",
    "errors = 0\n",
    "print(rounded[0][0])\n",
    "for iprobs_meanc1 in range(rounded.shape[0]):\n",
    "    if (rounded[i][0] != Y_test[i][0]):\n",
    "        errors += 1\n",
    "\n",
    "acc = 1 - (errors / Y_test.shape[0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ADA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78909390784455213"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble of Random Forests, getting great test accuracy to train time ratio\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=6)\n",
    "#clf.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Starting ADA\")\n",
    "# Now, use this as the base classifier for the adaboost classifier\n",
    "clf1 = AdaBoostClassifier(base_estimator=clf, n_estimators=100)\n",
    "clf1.fit(X_train, Y_train_1d)\n",
    "\n",
    "clf1.score(X_test, Y_test_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf1rf = clf1.predict_log_proba(c1X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.68288326, -0.70351755],\n",
       "       [-0.68075991, -0.70568982],\n",
       "       [-0.68275776, -0.70364567],\n",
       "       ..., \n",
       "       [-0.67401557, -0.71265196],\n",
       "       [-0.68345534, -0.70293387],\n",
       "       [-0.66086798, -0.72650318]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1rf[:, 0]\n",
    "import pandas as pd\n",
    "pd.DataFrame(clf1rf[:, 0]).to_csv(\"ag_c2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halfway\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "MLPClassifier doesn't support sample_weight.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-bb875ec4cfd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Halfway\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mclfDANK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclfMLP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mclfDANK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_1d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mclfDANK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test_1d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;31m# Check parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# Clear any previous fit results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_validate_estimator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_fit_parameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sample_weight\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             raise ValueError(\"%s doesn't support sample_weight.\"\n\u001b[1;32m--> 429\u001b[1;33m                              % self.base_estimator_.__class__.__name__)\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_boost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: MLPClassifier doesn't support sample_weight."
     ]
    }
   ],
   "source": [
    "# TODO: Make a Adaboosted MLP classifier. Should be dank af.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "clfMLP = MLPClassifier(hidden_layer_sizes=(512,256,128), early_stopping=True, learning_rate='adaptive')\n",
    "clfMLP.fit(X_train, Y_train)\n",
    "clfMLP.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM Classifier. This might be interesting to ADABoost. Takes a long time to train.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clfSVC = SVC()\n",
    "clfSVC.fit(X_test, Y_test_1d) # Train with smaller dataset to cuts down time.\n",
    "clfSVC.score(X_train, Y_train_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "MLPClassifier doesn't support sample_weight.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-a86f1083a8f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclfD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclfMLP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclfD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_1d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mclfD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test_1d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;31m# Check parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# Clear any previous fit results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_validate_estimator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_fit_parameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sample_weight\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             raise ValueError(\"%s doesn't support sample_weight.\"\n\u001b[1;32m--> 429\u001b[1;33m                              % self.base_estimator_.__class__.__name__)\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_boost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: MLPClassifier doesn't support sample_weight."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clfD = AdaBoostClassifier(base_estimator=clfMLP, n_estimators=100)\n",
    "clfD.fit(X_train, Y_train_1d)\n",
    "clfD.score(X_test, Y_test_1d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(probs_meanc1[:, 0]).to_csv(\"ag_c1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
