{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Seed the random number generator:\n",
    "np.random.seed(1)\n",
    "\n",
    "def load_data(filename, skiprows = 1):\n",
    "    \"\"\"\n",
    "    Function loads data stored in the file filename and returns it as a numpy ndarray.\n",
    "    \n",
    "    Inputs:\n",
    "        filename: given as a string.\n",
    "        \n",
    "    Outputs:\n",
    "        Data contained in the file, returned as a numpy ndarray\n",
    "    \"\"\"\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load all the data\n",
    "all = load_data('train_2008.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-f9cffa4ff3b4>, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-f9cffa4ff3b4>\"\u001b[1;36m, line \u001b[1;32m58\u001b[0m\n\u001b[1;33m    if (len(distinct_nums))\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "####### FEATURE ENGINEERING HERE #######\n",
    "# Below, we transform the real valued \n",
    "# data from the dataset to categorical \n",
    "# data if it should be. This should \n",
    "# vastly improve model performance.\n",
    "########################################\n",
    "import csv\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Split into features and labels\n",
    "Y = all[:, -1]\n",
    "X = all[:, :-1]\n",
    "\n",
    "featureNames = None\n",
    "with open('train_2008.csv', 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        featureNames = row\n",
    "        break\n",
    "\n",
    "# Delete all the columns which are identical among all data points, these only hurt our model \n",
    "shouldDelArr = [\"id\", \"QSTNUM\"]\n",
    "shouldDelete = np.all(X == X[0,:], axis = 0)\n",
    "for i in range(X.shape[1]):\n",
    "    ib = X.shape[1] - i - 1\n",
    "    if ((shouldDelete[ib] == True) or (featureNames[ib] in shouldDelArr)):\n",
    "        # delete the i-th columnif \n",
    "        X = np.delete(X, ib, 1)\n",
    "        featureNames = np.delete(featureNames, ib)\n",
    "\n",
    "newX = np.empty((X.shape[0],1))\n",
    "\n",
    "# These are the features which are NOT categorical; the rest I deemed were.\n",
    "real_valued_feats = [\"HWHHWGT\", \"GTCBSA\", \"GTCO\", \"PEAGE\", \"PEHRUSL1\", \"PEHRUSL2\", \"PUHROFF2\", \"PUHROT2\", \"PEHRACT2\", \n",
    "                     \"PELAYDUR\", \"PELKDUR\", \"PRUNEDUR\", \"PEERNHRO\", \"PEERNWKP\", \"PRNMCHLD\", \"PEHGCOMP\", \"PECYC\", \"PWCMPWGT\", \n",
    "                     \"PWFMWGT\", \"PWLGWGT\", \"PWORWGT\", \"PWSSWGT\", \"PWVETWGT\"]\n",
    "\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    feature = np.reshape(X[:, i], (X.shape[0], 1))\n",
    "    \n",
    "    #print(feature.shape)\n",
    "    if featureNames[i] in real_valued_feats:\n",
    "        newX = np.append(newX, feature, axis = 1)\n",
    "    else:\n",
    "        distinct_negs = []\n",
    "        distinct_nums = []\n",
    "        for val in feature:\n",
    "            if val[0] < 0:\n",
    "                if val[0] not in distinct_negs:\n",
    "                    distinct_negs.append(val[0])\n",
    "            if val[0] not in distinct_nums:\n",
    "                distinct_nums.append(val[0])\n",
    "        \n",
    "        distinct_nums.sort()\n",
    "        if (len(distinct_nums)> 100):\n",
    "            print(len(distinct_nums))\n",
    "        for i in range(feature.shape[0]):\n",
    "            feature[i][0] = distinct_nums.index(feature[i][0])\n",
    "        \n",
    "        feature = keras.utils.np_utils.to_categorical(feature)\n",
    "#         # delete the zero columns\n",
    "#         zero_cols = np.any(cat, axis=0)\n",
    "#         print\n",
    "#         for j in range(zero_cols.shape[0]):\n",
    "#             jb = zero_cols.shape[0] - j - 1\n",
    "#             if zero_cols[jb] == False:\n",
    "#                 cat = np.delete(cat, jb, axis=1)     \n",
    "        # Delete the negative cols\n",
    "        feature = np.delete(feature, np.s_[0:len(distinct_negs)], 1) \n",
    "        \n",
    "        # print(test.shape, newX.shape)\n",
    "        newX = np.append(newX, feature, axis = 1)\n",
    "        \n",
    "        \n",
    "# Delete the placeholder column and transpose\n",
    "newX = np.delete(newX, 0, 1)\n",
    "print(newX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into train and validation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(rows_train, rows_test):\n",
    "    columns_train  = np.transpose(rows_train)\n",
    "    columns_test = np.transpose(rows_test)\n",
    "    for i in range(columns_train.shape[0]):\n",
    "        col = columns_train[i]\n",
    "        mean = np.mean(col)\n",
    "        stddev = np.std(col)\n",
    "        if (stddev == 0):\n",
    "            stddev = 1\n",
    "        columns_train[i] = (col - mean) / stddev\n",
    "        columns_test[i] = (columns_test[i] - mean) / stddev\n",
    "    return np.transpose(columns_train), np.transpose(columns_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_1d = Y_train.copy()\n",
    "Y_test_1d = Y_test.copy()\n",
    "\n",
    "# First convert to real labels\n",
    "Y_train = keras.utils.np_utils.to_categorical(Y_train)\n",
    "Y_test = keras.utils.np_utils.to_categorical(Y_test)\n",
    "\n",
    "\n",
    "X_train, X_test = normalize(X_train.copy(), X_test.copy())\n",
    "\n",
    "# svd = TruncatedSVD(n_components=324, n_iter=20, random_state=42)\n",
    "# svd.fit(X_train)\n",
    "# X_train = svd.transform(X_train)\n",
    "# X_test = svd.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, BatchNormalization, LocallyConnected1D\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_bags = 2\n",
    "kf = KFold(n_splits=n_bags)\n",
    "models = []\n",
    "\n",
    "for i in range(n_bags):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(X_train.shape[1],), kernel_regularizer=regularizers.l2(0.000)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Dense(128, kernel_regularizer=regularizers.l2(0.000)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.3))\n",
    "    model.add(Dense(128, kernel_regularizer=regularizers.l2(0.000)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27483 samples, validate on 27483 samples\n",
      "Epoch 1/10\n",
      "27483/27483 [==============================] - 5s - loss: 0.5270 - acc: 0.7445 - val_loss: 0.4935 - val_acc: 0.7613\n",
      "Epoch 2/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4933 - acc: 0.7623 - val_loss: 0.4867 - val_acc: 0.7676\n",
      "Epoch 3/10\n",
      "27483/27483 [==============================] - 2s - loss: 0.4845 - acc: 0.7672 - val_loss: 0.4896 - val_acc: 0.7712\n",
      "Epoch 4/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4761 - acc: 0.7727 - val_loss: 0.4848 - val_acc: 0.7701\n",
      "Epoch 5/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4743 - acc: 0.7734 - val_loss: 0.4877 - val_acc: 0.7724\n",
      "Epoch 6/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4688 - acc: 0.7780 - val_loss: 0.4877 - val_acc: 0.7704\n",
      "Epoch 7/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4655 - acc: 0.7804 - val_loss: 0.4835 - val_acc: 0.7710\n",
      "Epoch 8/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4627 - acc: 0.7818 - val_loss: 0.4866 - val_acc: 0.7719\n",
      "Epoch 9/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4600 - acc: 0.7822 - val_loss: 0.4849 - val_acc: 0.7736\n",
      "Epoch 10/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4565 - acc: 0.7865 - val_loss: 0.4835 - val_acc: 0.7739\n",
      "Train on 27483 samples, validate on 27483 samples\n",
      "Epoch 1/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.5307 - acc: 0.7473 - val_loss: 0.4909 - val_acc: 0.7631\n",
      "Epoch 2/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4958 - acc: 0.7619 - val_loss: 0.4841 - val_acc: 0.7677\n",
      "Epoch 3/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4847 - acc: 0.7690 - val_loss: 0.4881 - val_acc: 0.7662\n",
      "Epoch 4/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4798 - acc: 0.7742 - val_loss: 0.4873 - val_acc: 0.7679\n",
      "Epoch 5/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4754 - acc: 0.7771 - val_loss: 0.4827 - val_acc: 0.7701\n",
      "Epoch 6/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4722 - acc: 0.7786 - val_loss: 0.4804 - val_acc: 0.7716\n",
      "Epoch 7/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4686 - acc: 0.7818 - val_loss: 0.4837 - val_acc: 0.7698\n",
      "Epoch 8/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4665 - acc: 0.7817 - val_loss: 0.4851 - val_acc: 0.7708\n",
      "Epoch 9/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4636 - acc: 0.7841 - val_loss: 0.4836 - val_acc: 0.7719\n",
      "Epoch 10/10\n",
      "27483/27483 [==============================] - 3s - loss: 0.4614 - acc: 0.7840 - val_loss: 0.4809 - val_acc: 0.7719\n"
     ]
    }
   ],
   "source": [
    "i =0\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "    Y_train_fold, Y_test_fold = Y_train[train_index], Y_train[test_index]\n",
    "    models[i].compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # Train the model, iterating on the data in batches of 64 samples\n",
    "    history = models[i].fit(X_train_fold, Y_train_fold, epochs=10, batch_size=64,\n",
    "                    validation_data=(X_test_fold, Y_test_fold))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9056/9701 [===========================>..] - ETA: 0s0.0\n",
      "0.7746624059375322\n"
     ]
    }
   ],
   "source": [
    "probs = np.empty((5,X_test.shape[0], 2))\n",
    "i = 0\n",
    "for model in models:\n",
    "    probs[i] = model.predict_proba(X_test)\n",
    "    i += 1\n",
    "    \n",
    "probs_mean = np.mean(probs, axis = 0)\n",
    "rounded = np.round(probs_mean)\n",
    "errors = 0\n",
    "print(rounded[0][0])\n",
    "for i in range(rounded.shape[0]):\n",
    "    if (rounded[i][0] != Y_test[i][0]):\n",
    "        errors += 1\n",
    "\n",
    "acc = 1 - (errors / Y_test.shape[0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ADA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.78517678589836104"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble of Random Forests, getting great test accuracy to train time ratio\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "#clf.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Starting ADA\")\n",
    "# Now, use this as the base classifier for the adaboost classifier\n",
    "clf1 = AdaBoostClassifier(base_estimator=clf, n_estimators=100)\n",
    "clf1.fit(X_train, Y_train_1d)\n",
    "\n",
    "clf1.score(X_test, Y_test_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74487166271518401"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Make a Adaboosted MLP classifier. Should be dank af.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clfMLP = MLPClassifier(hidden_layer_sizes=(128,128,128), early_stopping=True, learning_rate='adaptive')\n",
    "clfMLP.fit(X_train, Y_train)\n",
    "clfMLP.score(X_test, Y_test)\n",
    "\n",
    "# clfDANK = AdaBoostClassifier(base_estimator=clf, n_estimators=10)\n",
    "# clfDANK.fit(X_train, Y_train_1d)\n",
    "# clfDANK.score(X_test, Y_test_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM Classifier. This might be interesting to ADABoost. Takes a long time to train.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clfSVC = SVC()\n",
    "clfSVC.fit(X_test, Y_test_1d) # Train with smaller dataset to cuts down time.\n",
    "clfSVC.score(X_train, Y_train_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
