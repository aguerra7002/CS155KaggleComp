{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Seed the random number generator:\n",
    "np.random.seed(1)\n",
    "\n",
    "def load_data(filename, skiprows = 1):\n",
    "    \"\"\"\n",
    "    Function loads data stored in the file filename and returns it as a numpy ndarray.\n",
    "    \n",
    "    Inputs:\n",
    "        filename: given as a string.\n",
    "        \n",
    "    Outputs:\n",
    "        Data contained in the file, returned as a numpy ndarray\n",
    "    \"\"\"\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load all the data\n",
    "all = load_data('train_2008.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 PENATVTY\n",
      "145 PEMNTVTY\n",
      "143 PEFNTVTY\n",
      "109 PEHRUSLT\n",
      "262 PEIO1ICD\n",
      "123 PEIO2ICD\n",
      "(64667, 2942)\n"
     ]
    }
   ],
   "source": [
    "####### FEATURE ENGINEERING HERE #######\n",
    "# Below, we transform the real valued \n",
    "# data from the dataset to categorical \n",
    "# data if it should be. This should \n",
    "# vastly improve model performance.\n",
    "########################################\n",
    "import csv\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Split into features and labels\n",
    "Y = all[:, -1]\n",
    "X = all[:, :-1]\n",
    "\n",
    "featureNames = None\n",
    "with open('train_2008.csv', 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        featureNames = row\n",
    "        break\n",
    "\n",
    "# Delete all the columns which are identical among all data points, these only hurt our model \n",
    "shouldDelArr = [\"id\", \"QSTNUM\", \"PEIO1OCD\", \"PEIO2OCD\", \"PEHRACTT\"]\n",
    "shouldDelete = np.all(X == X[0,:], axis = 0)\n",
    "for i in range(X.shape[1]):\n",
    "    ib = X.shape[1] - i - 1\n",
    "    if ((shouldDelete[ib] == True) or (featureNames[ib] in shouldDelArr)):\n",
    "        # delete the i-th columnif \n",
    "        X = np.delete(X, ib, 1)\n",
    "        featureNames = np.delete(featureNames, ib)\n",
    "\n",
    "newX = np.empty((X.shape[0],1))\n",
    "\n",
    "# These are the features which are NOT categorical; the rest I deemed were.\n",
    "real_valued_feats = [\"HWHHWGT\", \"GTCBSA\", \"GTCO\", \"PEAGE\", \"PEHRUSL1\", \"PEHRUSL2\", \"PUHROFF2\", \"PUHROT2\", \"PEHRACT2\", \n",
    "                     \"PELAYDUR\", \"PELKDUR\", \"PRUNEDUR\", \"PEERNHRO\", \"PEERNWKP\", \"PRNMCHLD\", \"PEHGCOMP\", \"PECYC\", \"PWCMPWGT\", \n",
    "                     \"PWFMWGT\", \"PWLGWGT\", \"PWORWGT\", \"PWSSWGT\", \"PWVETWGT\", \"PRERNWA\", \"PRERNHLY\", \"PEERNH1O\", \"PEERNH2\", \n",
    "                     \"PEERN\", \"PEHRACT1\"]\n",
    "\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    feature = np.reshape(X[:, i], (X.shape[0], 1))\n",
    "    \n",
    "    #print(feature.shape)\n",
    "    if featureNames[i] in real_valued_feats:\n",
    "        newX = np.append(newX, feature, axis = 1)\n",
    "    else:\n",
    "        distinct_negs = []\n",
    "        distinct_nums = []\n",
    "        for val in feature:\n",
    "            if val[0] < 0:\n",
    "                if val[0] not in distinct_negs:\n",
    "                    distinct_negs.append(val[0])\n",
    "            if val[0] not in distinct_nums:\n",
    "                distinct_nums.append(val[0])\n",
    "        \n",
    "        distinct_nums.sort()\n",
    "        if (len(distinct_nums)> 100):\n",
    "            print(len(distinct_nums), featureNames[i])\n",
    "            \n",
    "        for i in range(feature.shape[0]):\n",
    "            feature[i][0] = distinct_nums.index(feature[i][0])\n",
    "        \n",
    "        feature = keras.utils.np_utils.to_categorical(feature)\n",
    "#         # delete the zero columns\n",
    "#         zero_cols = np.any(cat, axis=0)\n",
    "#         print\n",
    "#         for j in range(zero_cols.shape[0]):\n",
    "#             jb = zero_cols.shape[0] - j - 1\n",
    "#             if zero_cols[jb] == False:\n",
    "#                 cat = np.delete(cat, jb, axis=1)     \n",
    "        # Delete the negative cols\n",
    "        feature = np.delete(feature, np.s_[0:len(distinct_negs)], 1) \n",
    "        \n",
    "        # print(test.shape, newX.shape)\n",
    "        newX = np.append(newX, feature, axis = 1)\n",
    "        \n",
    "        \n",
    "# Delete the placeholder column and transpose\n",
    "newX = np.delete(newX, 0, 1)\n",
    "print(newX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split into train and validation\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(newX, Y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(rows_train, rows_test):\n",
    "    columns_train  = np.transpose(rows_train)\n",
    "    columns_test = np.transpose(rows_test)\n",
    "    for i in range(columns_train.shape[0]):\n",
    "        col = columns_train[i]\n",
    "        mean = np.mean(col)\n",
    "        stddev = np.std(col)\n",
    "        if (stddev == 0):\n",
    "            stddev = 1\n",
    "        columns_train[i] = (col - mean) / stddev\n",
    "        columns_test[i] = (columns_test[i] - mean) / stddev\n",
    "    return np.transpose(columns_train), np.transpose(columns_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_1d = Y_train.copy()\n",
    "Y_test_1d = Y_test.copy()\n",
    "\n",
    "# First convert to real labels\n",
    "Y_train = keras.utils.np_utils.to_categorical(Y_train)\n",
    "Y_test = keras.utils.np_utils.to_categorical(Y_test)\n",
    "\n",
    "\n",
    "X_train, X_test = normalize(X_train.copy(), X_test.copy())\n",
    "\n",
    "# svd = TruncatedSVD(n_components=324, n_iter=20, random_state=42)\n",
    "# svd.fit(X_train)\n",
    "# X_train = svd.transform(X_train)\n",
    "# X_test = svd.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, BatchNormalization, LocallyConnected1D\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_bags = 5\n",
    "kf = KFold(n_splits=n_bags)\n",
    "models = []\n",
    "\n",
    "for i in range(n_bags):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_shape=(X_train.shape[1],), kernel_regularizer=regularizers.l2(0.0005)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(Dense(512, kernel_regularizer=regularizers.l2(0.0005)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(.4))\n",
    "    model.add(Dense(128, kernel_regularizer=regularizers.l2(0.0005)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27483 samples, validate on 27483 samples\n",
      "Epoch 1/10\n",
      "27483/27483 [==============================] - 18s - loss: 1.2288 - acc: 0.7515 - val_loss: 1.0043 - val_acc: 0.7663\n",
      "Epoch 2/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.8497 - acc: 0.7753 - val_loss: 0.7519 - val_acc: 0.7750\n",
      "Epoch 3/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.6731 - acc: 0.7800 - val_loss: 0.6483 - val_acc: 0.7722\n",
      "Epoch 4/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.6030 - acc: 0.7795 - val_loss: 0.6115 - val_acc: 0.7733\n",
      "Epoch 5/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.5724 - acc: 0.7840 - val_loss: 0.5836 - val_acc: 0.7699\n",
      "Epoch 6/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.5561 - acc: 0.7839 - val_loss: 0.5792 - val_acc: 0.7735\n",
      "Epoch 7/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.5484 - acc: 0.7860 - val_loss: 0.5827 - val_acc: 0.7705\n",
      "Epoch 8/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.5497 - acc: 0.7895 - val_loss: 0.5927 - val_acc: 0.7747\n",
      "Epoch 9/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.5508 - acc: 0.7906 - val_loss: 0.5860 - val_acc: 0.7758\n",
      "Epoch 10/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.5464 - acc: 0.7913 - val_loss: 0.5872 - val_acc: 0.7736\n",
      "Train on 27483 samples, validate on 27483 samples\n",
      "Epoch 1/10\n",
      "27483/27483 [==============================] - 18s - loss: 1.2746 - acc: 0.7526 - val_loss: 1.0621 - val_acc: 0.7683\n",
      "Epoch 2/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.9072 - acc: 0.7797 - val_loss: 0.7976 - val_acc: 0.7715\n",
      "Epoch 3/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.7090 - acc: 0.7826 - val_loss: 0.6847 - val_acc: 0.7698\n",
      "Epoch 4/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.6208 - acc: 0.7869 - val_loss: 0.6241 - val_acc: 0.7682\n",
      "Epoch 5/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.5793 - acc: 0.7871 - val_loss: 0.5945 - val_acc: 0.7726\n",
      "Epoch 6/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.5618 - acc: 0.7877 - val_loss: 0.5799 - val_acc: 0.7699\n",
      "Epoch 7/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.5511 - acc: 0.7897 - val_loss: 0.5813 - val_acc: 0.7713\n",
      "Epoch 8/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.5497 - acc: 0.7917 - val_loss: 0.5901 - val_acc: 0.7655\n",
      "Epoch 9/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.5529 - acc: 0.7935 - val_loss: 0.5890 - val_acc: 0.7731\n",
      "Epoch 10/10\n",
      "27483/27483 [==============================] - 18s - loss: 0.5529 - acc: 0.7959 - val_loss: 0.6022 - val_acc: 0.7709\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n",
    "    Y_train_fold, Y_test_fold = Y_train[train_index], Y_train[test_index]\n",
    "    models[i].compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # Train the model, iterating on the data in batches of 64 samples\n",
    "    history = models[i].fit(X_train_fold, Y_train_fold, epochs=10, batch_size=64,\n",
    "                    validation_data=(X_test_fold, Y_test_fold))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9248/9701 [===========================>..] - ETA: 0s0.0\n",
      "0.7300793732604887\n"
     ]
    }
   ],
   "source": [
    "probs = np.empty((5,X_test.shape[0], 2))\n",
    "i = 0\n",
    "for model in models:\n",
    "    probs[i] = model.predict_proba(X_test)\n",
    "    i += 1\n",
    "    \n",
    "probs_mean = np.mean(probs, axis = 0)\n",
    "rounded = np.round(probs_mean)\n",
    "errors = 0\n",
    "print(rounded[0][0])\n",
    "for i in range(rounded.shape[0]):\n",
    "    if (rounded[i][0] != Y_test[i][0]):\n",
    "        errors += 1\n",
    "\n",
    "acc = 1 - (errors / Y_test.shape[0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ADA\n"
     ]
    }
   ],
   "source": [
    "# Ensemble of Random Forests, getting great test accuracy to train time ratio\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "#clf.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Starting ADA\")\n",
    "# Now, use this as the base classifier for the adaboost classifier\n",
    "clf1 = AdaBoostClassifier(base_estimator=clf, n_estimators=50)\n",
    "clf1.fit(X_train, Y_train_1d)\n",
    "\n",
    "clf1.score(X_test, Y_test_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halfway\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "MLPClassifier doesn't support sample_weight.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-bb875ec4cfd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Halfway\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mclfDANK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclfMLP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mclfDANK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_1d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mclfDANK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test_1d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;31m# Check parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# Clear any previous fit results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_validate_estimator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_fit_parameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sample_weight\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             raise ValueError(\"%s doesn't support sample_weight.\"\n\u001b[1;32m--> 429\u001b[1;33m                              % self.base_estimator_.__class__.__name__)\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_boost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: MLPClassifier doesn't support sample_weight."
     ]
    }
   ],
   "source": [
    "# TODO: Make a Adaboosted MLP classifier. Should be dank af.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "clfMLP = MLPClassifier(hidden_layer_sizes=(512,256,128), early_stopping=True, learning_rate='adaptive')\n",
    "clfMLP.fit(X_train, Y_train)\n",
    "clfMLP.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM Classifier. This might be interesting to ADABoost. Takes a long time to train.\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clfSVC = SVC()\n",
    "clfSVC.fit(X_test, Y_test_1d) # Train with smaller dataset to cuts down time.\n",
    "clfSVC.score(X_train, Y_train_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "MLPClassifier doesn't support sample_weight.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-a86f1083a8f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclfD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclfMLP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclfD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_1d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mclfD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test_1d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;31m# Check parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# Clear any previous fit results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_validate_estimator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_fit_parameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sample_weight\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m             raise ValueError(\"%s doesn't support sample_weight.\"\n\u001b[1;32m--> 429\u001b[1;33m                              % self.base_estimator_.__class__.__name__)\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_boost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: MLPClassifier doesn't support sample_weight."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clfD = AdaBoostClassifier(base_estimator=clfMLP, n_estimators=100)\n",
    "clfD.fit(X_train, Y_train_1d)\n",
    "clfD.score(X_test, Y_test_1d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
